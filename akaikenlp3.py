# -*- coding: utf-8 -*-
"""AkaikeNLP3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DVZVfEMukciSOYD7m6yllN1Y_g1Kdg2U

**Problem Statement:**

•	Objective 1: Get the most frequent entities from the tweets.

•	Objective 2: Find out the sentiment/polarity of each author towards each of the entities.

**Technology/Tools used:**

•	Spacy for tokenization, stop words removal, named entity recognition.

•	Scispacy (“en_ner_bc5cdr_md” module having labels “Entity” for named entity recognition. (medical or bi medical named entities)

•	“en_core_sci_lg” module is used to cater to clinical dataset i.e; “tweets.json”

•	Pandas for analyzing the dataset.

•	Json to reads the dataset to Dataframe.

•	Natural Language Processing library “SentimentIntensityAnalyzer”, Valence Aware Dictionary or Vader for sentiment Reasoning.

**Approach followed:**

Step 1:  Installed spacy.

Step 2:  Imported and loaded scispacy and its biomedical module “en_ner_bc5cdr_md”.

Step 3:  “Json” format read to Dataframe using orientation “index”

Step 4:  Cleaned “tweet_text” column. 

•	No duplicates or missing values in the Dataframe.

•	Removed punctuations, emoji s and digits,

•	Removed hyper link and hashtags, 

•	Removed words less than 2 characters

•	Changed to lower case, 

•	Removed extra trailing spaces, 

•	Created tokens, 

•	Removed stop words

Step 5:  Extracted entity from “cleaned_tweet_text” column.

Step 6:  Frequency of entities (solution to problem statement 1)

Step 7:  Calculated polarity on each entity.

Step 8:  Authors' sentiment analysis (solution to problem statement 2)

**Approach to solve the problem :**

•	Spacy module “en_core_web_sm” did not recognise context related entity, tried few clinical module but  “en_ner_bc5cdr_md” biomedical  module of scispacy solved this problem.

**Conclusion:**

•	Data Cleansing and Entities Extraction were a crucial parts of the solution. Removing unnecessary elements from “tweet_text” consumed most of the time.

•	Entities Extraction is a key point in the solution. Finding the right entities defines extracting the right phrases from the tweets on which the core meaning of tweets relies on.

•	Then, there comes extracting the sentiments of each entities from the tweets with the help of "NLTK package SentimentIntensityAnalyzer" .

# **Installing required Packages**
"""

!pip install spacy

#medical or bi medical named entities
!pip install scispacy

#en_ner_bc5cdr_md - chemical disease relation (scispacy module)
#!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_bc5cdr_md-0.5.1.tar.gz

#en_core_sci_lg
!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_lg-0.5.1.tar.gz

#en_ner_craft_md
#!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_craft_md-0.5.1.tar.gz

#en_ner_jnlpba_md
#!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_jnlpba_md-0.5.1.tar.gz

#en_ner_bionlp13cg_md
#!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_bionlp13cg_md-0.5.1.tar.gz

"""# **Importing required Library**"""

#Importing Pandas for data analysis and work with dataframe
import pandas as pd
#Importing numpy to working with arrays 
import numpy as np 
#Importing json to read the .json file
import json
#Importing re for regular expression operation
import re

#importing scipacy packages
import spacy
import scispacy
#import en_ner_bc5cdr_md
#import en_ner_craft_md
#import en_ner_bionlp13cg_md
#import en_ner_jnlpha_md
import en_core_sci_lg

#importing _________
#import en_core_web_sm

import nltk
nltk.download('averaged_perceptron_tagger')

import nltk
nltk.download('stopwords')
nltk.download('brown')
nltk.download('wordnet')
nltk.download('brown')
nltk.download('punkt')

"""# **Loading necessary Modules**"""

#loading modules
#ner_bc = en_ner_bc5cdr_md.load()
#ner_cr = en_ner_craft_md.load()
#ner_bi = en_ner_bionlp13cg_md.load()
#ner_jn = en_ner_jnlpha_md.load()
#ner_web = en_core_web_sm.load()
ner_sci = en_core_sci_lg.load()

"""# **Reading the .json file to dataframe**"""

df = pd.read_json('/content/tweets.json',orient ='index')
df = df.reset_index(drop = False)
df

"""# **Data Preprocessing (Cleaning the data)**"""

#checking missing values
df.isnull().sum()

#checking for duplicates
df.duplicated().sum()

#removing hashtags (word starting with #)
df['tweet_text'] = df['tweet_text'].replace(r'#[\w]*', '', regex=True)
df["tweet_text"][0]

#removing https links
df['tweet_text'] = df['tweet_text'].replace(r'http\S+', '', regex=True).replace(r'www\S+', '', regex=True)
df["tweet_text"][0]

# Replacing punctuations with space
df['tweet_text'] = df['tweet_text'].str.replace("[^a-zA-Z]", " ",regex = True)
df["tweet_text"][0]

# make entire text lowercase
df['tweet_text'] = [row.lower() for row in df['tweet_text']]
df["tweet_text"][0]

# #remove words with len < 3
df['tweet_text'] = df['tweet_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))
df["tweet_text"][0]

#tokenization
#df["cleaned_tweet_text"] = df["tweet_text"].apply(lambda x: [t.text for t in ner_bc.tokenizer(x)])
#df
#df["cleaned_tweet_text"] = df["tweet_text"].apply(lambda x: ner_bc.tokenizer(x))
#df
df["cleaned_tweet_text"] = df["tweet_text"].apply(lambda x: ner_sci.tokenizer(x))
df

#stopwords removal
#df["cleaned_tweet_text"] = df["cleaned_tweet_text"].apply(lambda x: [token.text for token in ner_bc(x) if not token.is_stop])
#df
df["cleaned_tweet_text"] = df["cleaned_tweet_text"].apply(lambda x: [token.text for token in ner_sci(x) if not token.is_stop])
df

df["cleaned_tweet_text"] = df["cleaned_tweet_text"].astype(str)

df["cleaned_tweet_text"][0]

df['cleaned_tweet_text'] = df['cleaned_tweet_text'].str.replace("[^a-zA-Z]", " ",regex = True)
df["cleaned_tweet_text"][0]

"""# ***SciSpacy Named Entity Recognition(NER)***

***Application on DataFrame***
"""

print(list(ner_sci.component_names))

print(list(ner_sci.get_pipe("ner").labels))

def add_bc(abstractList, doiList):
    i = 0
    table= {"Author":[], "Entity":[], "Class":[]}
    for doc in ner_sci.pipe(abstractList):
        doi = doiList[i]
        for x in doc.ents:
          table["Author"].append(doi)
          table["Entity"].append(x.text)
          table["Class"].append(x.label_)
        i +=1
    return table

#meta_df = pd.read_csv("/content/sample.csv")

#Sort out blank abstracts
#df = meta_df.dropna(subset=['abstract'])

#Create lists
doiList = df['tweet_author'].tolist()
abstractList = df['cleaned_tweet_text'].tolist()

#Add all entity value pairs to table (run one at a time, each ones takes ~20 min)
table = add_bc(abstractList, doiList)

# table = add_bc(abstractList, doiList)

# table = add_bi(abstractList, doiList)

# table = add_jn(abstractList, doiList)

#Turn table into an exportable CSV file (returns normalized file of entity/value pairs)
ner_df = pd.DataFrame(table)
#trans_df.to_csv ("Entity_pairings.csv", index=False)

#trans_df = pd.DataFrame(table)
ner_df[:30]
#trans_df.to_csv("Entity.csv",index = False)

ner_df.duplicated().value_counts()

ner_df.drop_duplicates(inplace = True)
ner_df

ner_df[ner_df["Entity"] == "na"]
  
#ner_df_cleaned = ner_df[ner_df['Entity'] != "na"][["Author","Entity","Class"]]
#ner_df_cleaned
ner_df_cleaned = ner_df

freq = pd.DataFrame(ner_df_cleaned['Entity'].value_counts()).reset_index().rename(columns = {"index":"Entity","Entity":"Frequency"})
freq[:20]
freq.to_csv("objective1.csv", encoding='utf-8',index = False)

df5 = pd.read_csv("/content/objective1.csv")
df5

"""# ***Part II - Sentimental analysis/Polarity***"""

polarity_df = ner_df_cleaned.copy()
polarity_df

#polarity = en_ner_bc5cdr_md.load()
#polarity = en_core_web_sm.load()
polarity = en_core_sci_lg.load()

!pip install vaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from nltk.sentiment.util import *

#Sentiment Analysis
SIA = SentimentIntensityAnalyzer()

polarity_df["Entity"]= polarity_df["Entity"].astype(str)
# Applying Model, Variable Creation
polarity_df['Polarity Score'] = polarity_df["Entity"].apply(lambda x:SIA.polarity_scores(x)['compound'])
polarity_df['Neutral Score'] = polarity_df["Entity"].apply(lambda x:SIA.polarity_scores(x)['neu'])
polarity_df['Negative Score'] = polarity_df["Entity"].apply(lambda x:SIA.polarity_scores(x)['neg'])
polarity_df['Positive Score'] = polarity_df["Entity"].apply(lambda x:SIA.polarity_scores(x)['pos'])


# Converting 0 to 1 Decimal Score to a Categorical Variable
polarity_df['overall polarity']=''
polarity_df.loc[polarity_df['Polarity Score'] > 0,'overall polarity']='Positive'
#polarity_df.loc[polarity_df['Polarity Score'] == 0,'overall polarity']='Neutral'
polarity_df.loc[polarity_df['Polarity Score'] <= 0,'overall polarity']='Negative'
polarity_df[:30]

polarity_df["overall polarity"].value_counts()

polarity_df.columns

del polarity_df["Polarity Score"]
#,"Neutral Score","Negative Score","Positive Score","Class"])

del polarity_df["Neutral Score"]

del polarity_df["Negative Score"]

del polarity_df["Positive Score"]

del polarity_df["Class"]

polarity_df

polarity_df.to_csv("objective2.csv", encoding='utf-8',index = False)